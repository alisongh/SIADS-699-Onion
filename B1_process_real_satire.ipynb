{"cells":[{"cell_type":"markdown","metadata":{"id":"yL0eSdfVFnwR"},"source":["# Real vs Satire Data Processing (Article level)\n","\n","1. Read files by article\n","2. Lowercase texts\n","3. Remove stopwords and punctuations\n","4. Remove duplications\n","5. Remove proper nouns\n","6. Remove all inreadable codes\n","7. Generate topics\n","8. Save as CSV files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MO1Mu-t2yNfO"},"outputs":[],"source":["pip install empath"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"542b4523ef5645f1829f651c3e004d7c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2510,"execution_start":1668308817775,"id":"sfBtOM5SkBuz","source_hash":"11e51dae","tags":[]},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from empath import Empath\n","from google.colab import drive\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","pd.options.mode.chained_assignment = None"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"53b9e466becb4407b0a04eaab3cc3887","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1306,"execution_start":1668308820289,"id":"jjSBXA0WkBu2","source_hash":"72f98587","tags":[]},"outputs":[],"source":["nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","stops = stopwords.words(\"english\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9m860OhkBu3"},"outputs":[],"source":["# Uncomment if connecting to Google Drive\n","# Run this cell and select your UMich Google account in the pop-up\n","\n","# drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7c86952c36fd45b5b27c406a49a34a3e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1668308821528,"id":"pE07Z2lKkBu4","source_hash":"f370df54","tags":[]},"outputs":[],"source":["real_path = \"data\\raw\\real_satire\\true\"\n","satire_path = \"data\\raw\\real_satire\\satire\"\n","true_train_list = [\"true_train_1.txt\", \"true_train_2.txt\", \"true_train_3.txt\", \"true_train_4.txt\", \"true_train_5.txt\", \"true_train_6.txt\"]\n","true_test_list = [\"true_test_1.txt\", \"true_test_2.txt\"]\n","true_dev_list = [\"true_validation_1.txt\", \"true_validation_2.txt\"]\n","satire_train_list = [\"satire_train.txt\"]\n","satire_test_list = [\"satire_test.txt\"]\n","satire_dev_list = [\"satire_dev.txt\"]"]},{"cell_type":"markdown","metadata":{"id":"tG8DzP0BkBu5"},"source":["https://huggingface.co/docs/transformers/model_doc/roberta"]},{"cell_type":"markdown","metadata":{"id":"u_7F5CvckBu6"},"source":["About the data:\n","\n","\"We omit headline, creation time, and author information so this work concentrates on the satire in the article body.\""]},{"cell_type":"markdown","metadata":{"id":"-j2DGnMmkBu7"},"source":["## Article level"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SW4zKf0HkBu8"},"outputs":[],"source":["def read_files(folder_path, file_list, real=True):\n","    \"\"\"\n","    Reads in files from a folder and returns a dataframe with the text and the label\n","    \n","    Parameters\n","    ----------\n","    folder_path : str\n","        The path to the folder containing the files\n","    file_list : list\n","        A list of the files to read in\n","    real : bool\n","        Whether the files are real or satire\n","    \"\"\"\n","    print(\"Preprocessing...\")\n","    tmp_list = []\n","    for file in file_list:\n","        with open(folder_path + file, \"r\") as f:\n","            file = f.read()\n","            # convert all strings to lowercases\n","            text_string = file.lower()\n","            # split the content of the file into a list of articles\n","            tmp_list.append(text_string.split(\"******\"))\n","    \n","    content_dict = {}\n","    content_dict[\"content\"] = None\n","    content_df = pd.DataFrame(columns=[\"content\"])  \n","    print(\"Reading files...\")\n","    for anArticle in tmp_list:\n","        for num in range(len(anArticle)-1):\n","            # the last line of content is empty, so we reduce the length by 1\n","            anArticle[num] = anArticle[num].replace(\"? ?\", \"\")\n","            anArticle[num] = anArticle[num].replace(\"u . s .\", \"us\")\n","            anArticle[num] = anArticle[num].replace(\"van gaal\", \"\")\n","            anArticle[num] = anArticle[num].replace(\"barack obama\", \"\")\n","            # remove unreadable codes and proper names\n","\n","            if anArticle[num].split(\"\\n\")[0] != \"\":\n","                content_dict[\"content\"] = ' '.join(anArticle[num].split(\"\\n\")[1:-1])\n","            else: \n","                content_dict[\"content\"] = ' '.join(anArticle[num].split(\"\\n\")[2:-1])\n","            content_df = content_df.append(content_dict, ignore_index=True)\n","    # create a new column to indicate whether the article is real or satire\n","    # 1 for real, 0 for satire\n","    if real:\n","        content_df[\"label\"] = 1\n","        print(\"Read {} real articles\".format(len(content_df)))\n","        print(\"The real article data is loaded successfully!\")\n","        return content_df\n","    else:\n","        content_df[\"label\"] = 0\n","        print(\"Read {} satire articles\".format(len(content_df)))\n","        print(\"The satire article data is loaded successfully!\")\n","        return content_df\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJn_ToB2kBu9"},"outputs":[],"source":["real_train_data = read_files(real_path, true_train_list, real=True)\n","satire_train_data = read_files(satire_path, satire_train_list, real=False)\n","real_test_data = read_files(real_path, true_test_list, real=True)\n","satire_test_data = read_files(satire_path, satire_test_list, real=False)\n","real_dev_data = read_files(real_path, true_dev_list, real=True)\n","satire_dev_data = read_files(satire_path, satire_dev_list, real=False)\n","\n","test_df = pd.concat([real_test_data, satire_test_data], ignore_index=True)\n","dev_df = pd.concat([real_dev_data, satire_dev_data], ignore_index=True)\n","train_df = pd.concat([real_train_data, satire_train_data], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfURiJe3kBu9"},"outputs":[],"source":["# proportion of real and satire articles in the training set\n","train_df.reset_index().groupby('label')['content'].count().plot(kind='bar', title=\"Real vs Satire\", figsize=(10, 5))\n","plt.xlabel(\"Label: 1 for Real, 0 for Satire\")\n","plt.ylabel(\"Number of Articles\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMpN43zMkBu-"},"outputs":[],"source":["def remove_duplicate(df, column_name, remove=True):\n","    \"\"\"\n","    Removes duplicate articles from the dataframe\n","    \n","    Parameters\n","    ----------\n","    df : pandas dataframe\n","        The dataframe to remove duplicates from\n","    column_name : str\n","        The name of the column to remove duplicates from\n","    remove : bool\n","        Whether to remove duplicates or not\n","    \"\"\"\n","    # check duplicates\n","    print(\"There are {} duplicates in the dataset\".format(df.duplicated().sum()))\n","    duplicate_df = df[df.duplicated(subset=column_name)]\n","    if remove:\n","        if df.duplicated().sum() > 0:\n","            updated_df = df.drop_duplicates(subset=[column_name], keep=False, inplace=False)\n","            print(\"The duplicate articles are removed successfully!\")\n","            return duplicate_df, updated_df\n","        else: \n","            print(\"There is no duplicate article in the dataset\")\n","            return duplicate_df\n","    else:\n","        print(\"The duplicate articles are NOT removed!\")\n","        return duplicate_df\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05tPrOkukBu-"},"outputs":[],"source":["duplicate_real_train_df, updated_real_train_df = remove_duplicate(real_train_data, \"content\", remove=True)\n","duplicate_satire_train_df, updated_satire_train_df = remove_duplicate(satire_train_data, \"content\", remove=True)\n","duplicate_train_df, updated_train_df = remove_duplicate(train_df, \"content\", remove=True)\n","duplicate_real_test_df, updated_real_test_df = remove_duplicate(real_test_data, \"content\", remove=True)\n","duplicate_satire_test_df, updated_satire_test_df = remove_duplicate(satire_test_data, \"content\", remove=True)\n","duplicate_test_df, updated_test_df = remove_duplicate(test_df, \"content\", remove=True)\n","duplicate_real_dev_df, updated_real_dev_df = remove_duplicate(real_dev_data, \"content\", remove=True)\n","duplicate_satire_dev_df, updated_satire_dev_df = remove_duplicate(satire_dev_data, \"content\", remove=True)\n","duplicate_dev_df, updated_dev_df = remove_duplicate(dev_df, \"content\", remove=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEXiuSF6kBu_"},"outputs":[],"source":["def remove_stop_punct(df, col_name, updated_col_name, re):\n","    \"\"\"\n","    Removes stop words and punctuations from the dataframe\n","\n","    Parameters\n","    ----------\n","    df : pandas dataframe\n","        The dataframe to remove stop words and punctuations from\n","    col_name : str\n","        The name of the column to remove stop words and punctuations from\n","    updated_col_name : str\n","        The name of the column to store the updated content\n","    re : str\n","        Regex for removing punctuations\n","    \"\"\"\n","    print(\"Removing stop words and punctuations...\")\n","    df[updated_col_name] = df[col_name].apply(lambda x: ' '.join([word for word in x.split() if word not in (stops)]))\n","    df[updated_col_name] = df[updated_col_name].str.replace(re,'')\n","    print(\"Stop words and punctuations are removed successfully!\")\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPKGdM22kBu_"},"outputs":[],"source":["updated_real_train_df = remove_stop_punct(updated_real_train_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_satire_train_df = remove_stop_punct(updated_satire_train_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_train_df = remove_stop_punct(updated_train_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_real_test_df = remove_stop_punct(updated_real_test_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_satire_test_df = remove_stop_punct(updated_satire_test_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_test_df = remove_stop_punct(updated_test_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_real_dev_df = remove_stop_punct(updated_real_dev_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_satire_dev_df = remove_stop_punct(updated_satire_dev_df, \"content\", \"clean_content\", '[^\\w\\s]')\n","updated_dev_df = remove_stop_punct(updated_dev_df, \"content\", \"clean_content\", '[^\\w\\s]')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8c9oYWRkBvA"},"outputs":[],"source":["# concat dev data to train and test data\n","updated_test_df = pd.concat([updated_dev_df[:8396], updated_test_df], ignore_index=True)\n","updated_real_test_df = pd.concat([updated_real_dev_df[:1550], updated_real_test_df], ignore_index=True)\n","updated_satire_test_df = pd.concat([updated_satire_dev_df[:9947], updated_satire_test_df], ignore_index=True)\n","\n","updated_train_df = pd.concat([updated_dev_df[8397:], updated_train_df], ignore_index=True)\n","updated_real_train_df = pd.concat([updated_real_dev_df[1551:], updated_real_train_df], ignore_index=True)\n","updated_satire_train_df = pd.concat([updated_satire_dev_df[9948:], updated_satire_train_df], ignore_index=True)\n","\n","print(\"The number of real articles in the test dataset is {}\".format(len(updated_real_test_df)))\n","print(\"The number of satire articles in the test dataset is {}\".format(len(updated_satire_test_df)))\n","\n","\n","print(\"The number of real articles in the train dataset is {}\".format(len(updated_real_train_df)))\n","print(\"The number of satire articles in the train dataset is {}\".format(len(updated_satire_train_df)))\n","\n","print(\"---------------------------------------------\")\n","print(\"The number of articles in the test dataset is {}\".format(len(updated_test_df)))\n","print(\"The number of articles in the train dataset is {}\".format(len(updated_train_df)))"]},{"cell_type":"markdown","metadata":{"id":"Bty-bH4VkBvA"},"source":["## Generate topics for each article"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwIBe3vhkBvA"},"outputs":[],"source":["def get_topic(df):\n","    \"\"\"\n","    Extracts the topic of the article\n","    \n","    Parameters\n","    ----------\n","    df : pandas dataframe\n","        The dataframe to extract the topic from\n","    \"\"\"\n","    lexicon = Empath()\n","    print(\"Getting topics...\")\n","    df['topic'] = None\n","    for i in range(len(df)):\n","        df['topic'][i] = sorted(lexicon.analyze(df['clean_content'][i]).items(), key=lambda x: x[1], reverse=True)[0][0]\n","    print(\"Topics are extracted successfully!\")\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTOZxvxqkBvA"},"outputs":[],"source":["updated_train_df = get_topic(updated_train_df)\n","updated_test_df = get_topic(updated_test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1tHRVWP601v"},"outputs":[],"source":["updated_train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_A38kGnJ8yq7"},"outputs":[],"source":["# concat train and test data as the whole dataset\n","df = pd.concat([updated_train_df, updated_test_df], ignore_index=True)\n","# plot topic distribution for top 15 topics\n","sns.set_theme(style=\"darkgrid\")\n","df['topic'].value_counts()[:15].plot(kind='bar', figsize=(8, 5), title='Topic Distribution')"]},{"cell_type":"markdown","metadata":{"id":"0MNtuyfw826k"},"source":["Top 15 topics are \"sports\", \"government\", \"crime\", \"business\", \"leader\", \"politics\", \"family\", \"play\", \"health\", \"children\", \"money\", \"war\", \"school\", \"military\", \"air_travel\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P47apidm8zOL"},"outputs":[],"source":["sorted_df = df[df['topic'].isin([\"sports\", \"government\", \"crime\", \"business\", \"leader\", \"politics\", \"family\", \"play\", \"health\", \"children\", \"money\", \"war\", \"school\", \"military\", \"air_travel\"])]\n","fig, ax = plt.subplots(figsize=(12, 5))\n","sns.countplot(x=\"topic\", data=sorted_df, order=sorted_df.topic.value_counts().iloc[:15].index, hue=\"label\", palette=\"Set2\")\n","plt.legend(loc='upper right', labels=['satire', 'real'])\n","plt.xticks(rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"Cp3wxZTykBvB"},"source":["## Save as CSV files\n","\n","Uncomment below code if saving as CSV files is needed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbAR4-cnkBvB"},"outputs":[],"source":["# updated_train_df.to_csv(\"data\\processed\\real_satire\\updated_train_df.csv\", index=False)\n","# updated_test_df.to_csv(\"data\\processed\\real_satire\\updated_test_df.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0hyYxm5kBvB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"HV1UHTMHkBvB"},"source":["----"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8fdedbd697e162e6a50f6d6091fa3a0e7608a0434d4cae79e96476424c027db7"}}},"nbformat":4,"nbformat_minor":0}
